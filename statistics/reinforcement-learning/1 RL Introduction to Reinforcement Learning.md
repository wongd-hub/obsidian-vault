---
title: "1 RL Introduction to Reinforcement Learning"
---

#course_google-deepmind-reinforcement-learning #reinforcement-learning
## About reinforcement learning

![[Pasted image 20230816152354.png]]

- *Reinforcement learning* (RL) sits at the intersection of many different fields of science. Many different fields try to understand the optimal way to make decisions, and these all relate to reinforcement learning.

![[Pasted image 20230816152543.png]]

- How is RL distinct from supervised and unsupervised learning?
    - There is no supervisor, only a reward signal
    - Feedback is delayed, possibly by several steps - and is not instantaneous
    - Time actually matters, we're looking at sequential processes in which an agent moves throughout a world which means non-i.i.d data
    - The agent's actions can influence its environment and therefore the subsequent data it sees

> [!tip] Examples of RL use cases
> - **Fly stunt manoeuvres in a helicopter** - a reward if the manoeuvre is performed successfully with no crashes
> - **Defeat the world champion in Backgammon or play Atari games better than humans** - good reward if a game is won
>    - Don't even need to tell it the rules of the game, just set up the reward structure
> - **Manage an investment portfolio** - the reward might be financial returns
> - **Make a humanoid robot walk** - rewards for making progress

## The reinforcement learning problem
### Reward

- A *reward* $R_t$ - a scalar feedback signal. Indicates how well the agent is doing at step *t*. The agent's job is to maximise cumulative reward.
    - The overarching goal is to select actions to maximise total future reward.
        - Actions might have long term consequences
        - Reward may be delayed
        - It may be better to sacrifice immediate reward to gain more long-term reward, strategic thinking is part of the process
    - What if the goal is to perform an action in the fastest amount of time? Typically what's done is to define the reward as `-1` per time step.
    - You can use multi-faceted reward systems, but you can generally represent these in a summarised fashion as a scalar reward.

> [!info] Definition: Reward Hypothesis
> All goals can be described by the *maximisation* of *expected cumulative reward*.
> 
> - Reinforcement learning is based on this hypothesis.

> [!tip] Examples of $R_t$
> - **Fly stunt manoeuvres in a helicopter**: +ve for following desired trajectory, (large) -ve for crashing
>     - Re-fueling itself might prevent a crash in several hours (stop and lose a small amount of reward for not following path to mitigate a crash later due to no fuel)
> - **Defeat the world champion at Backgammon**: no intermediate rewards, but at end of game give a +/-ve signal based on winning/losing the game
> - **Manage an investment portfolio signal**: +ve reward for each $ in bank
>     - Investment decisions might take months to come to fruition (delayed reward)
> - **Controlling a power station**: +ve reward for producing power, -ve reward for exceeding safety thresholds
> - **Making a humanoid robot walk**: +ve reward for forward motion, (large) -ve reward for falling over
> - **Making a car go around a track**: -ve reward for exceeding track limits/moving x% away from designated racing line, -ve reward for every second taken to get around the track
>     - The former needs to be big enough that the optimal strategy isn't to exceed track limits to gain time

### Agent and the environment

![[Pasted image 20230816155812.png]]

- The brain represents the *agent*, this is what we're building. Our agent will be composed of algorithms that will be able to respond to stimuli and take actions (such as move controls, or make investments). So at each step the agent will:
    - Execute action $A_t$
    - Receive observation $O_t$
    - Receive scalar reward $R_t$

- The *environment* is represented by the world (what's on the other side of the agent). There will be a loop over time, where in every step the agent sees a snapshot of the world at this moment generated by the environment. We have no control over the environment except through the agent's action $A_t$. So the environment will:
    - Receive action $A_t$
    - Emit observation $O_t$
    - Emit scalar reward $R_t$
- If we have a *multi-agent* system, the other agents can be seen as part of this environment to any given agent.

- The *history* is the sequence of observations, actions, and rewards (all observable variables up to time *t*; the sensorimotor stream of the agent). This is everything the agent has seen so far.
$$H_T = A_1, O_1, R_1, \dots, A_t, O_t, R_t$$
- What happens next depends on this history.
    - Our goal is to build a mapping (i.e. an algorithm) from one of these histories to picking the next action. the agent will use this to select the next action.
    - The environment will select the observations it emits and the rewards it provides at each time stamp.
    - The history isn't very useful though because it is typically long, we want to have agents that have long lives that can deal with microsecond interactions; so what we talk about instead is *state*.

### State

- *State* is a summary of the information that's used to determine what happens next. We replace the history with some concise summary with all the info we need to act. Formally, it is a function of the history:
$$S_t=f\left(H_t\right)$$
- There are three definitions of state; what do they mean and how do they relate to each other?

    - The *environment* state ($S_t^e$); this is the information that's used in the environment to determine what happens next, i.e. the next observation/reward. 
        - There will be some set of numbers that describes the process that drives what happens next. The summarisation of that which is fed to the agent is the environment state.
        - The environment state is not usually visible to the agent; the agent can only see what is provided by the environment in the observation.
        - Even if we could see this information, sometimes it may not be the right information we'd want to use to make the decision (i.e. does the atomic makeup of the game world help the agent make better decisions? Maybe its own subjective view of the environment may be better)

    - The *agent* state ($S_t^a$) captures what's happened to the agent so far, summarises it all into useful information, and it uses these numbers to pick the next action.
        - Our decision is how to process those observations and what to remember/throw away.
        - It is the information used by the RL algorithm and can be any function of the history $S_t^a=\left(H_t\right)$.

    - The *information* state (a.k.a. *Markov* state - a concept from information theory) contains all useful information from history.
        - By definition, the environment state is Markov. $S_t^e$ at any given moment is Markov since that's what its using to pick the next observation it will emit and the reward.
        - By definition, the history $H_t$ is Markov. If we retain the whole history of everything and make our decisions on the entire history, the entire history contains as much information as the entire history.
        - It's always possible to come up with some Markov state, the question is how do we find a useful representation in practice.

> [!info] Definition: Markov State
> A state $S_t$ is *Markov* iff:
> $$\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1, \dots, S_t]$$
> In other words, the probability of the next state given the state you're in is the same as if you showed all of the previous states to the system.
> 
> i.e. This is a *memoryless* system. The current state fully characterises the distribution of over future actions/observations/rewards.
> $$H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}$$

> [!tip] Example: Helicopter manoeuvres
> For the helicopter, its current velocity, orientation, wind speed, etc. may form a rough Markov state for the agent. When it knows this, it doesn't matter where it was before this, from this it knows where it will be in the next moment.
> 
> An imperfect state might be if you only had the position but not the velocity. This is not Markov because from this state, you don't know where it will be in future since you don't know how fast it's moving. You have to look back in time to figure out what its velocity is and what its momentum will be.

> [!tip] Example: The rat
> ![[Pasted image 20230816163729.png]]
> Take for example, a rat in an experiment. Say that it experiences the three sequences shown above.
> - If you were the agent and used the last three items in the sequence as your state, you would believe that you were about to be electrocuted.
> - If you chose your agent state to be the count of lights, bells, and levers, you'd expect that the cheese would appear.
> - If your agent state was the complete sequence we wouldn't know what happens next.
> 
> ==What we believe will happen next depends on our representation of state.==

- A *fully observable environment* is one in which the agent *directly observes* the environment state. This is a nice case, the agent sees all the numbers in the environment state. i.e. $O_t=S_t^a=S_t^e$.
    - When we have this situation, this is formally a *Markov decision process* (MDP). We'll discuss this in the next lecture and the majority of the course.

- *Partial observability* describes the situation in which the agent *indirectly observes* the environment. e.g. A robot with camera vision isn't told its absolute location, and poker agent only observes the public cards.
    - Now the agent must construct its own state representation $S_t^a$ that's distinct from the environment state. There are many ways to do this:
        - **Complete history** - naive approach: $S_t^a=H_t$
        - **Build beliefs of the environment state** - the Bayesian approach. Don't know what's happening in the environment but going to keep a probability distribution over where we think we are in the environment: $S_t^a = (\mathbb{P}[S_t^e = s^1], \ldots, \mathbb{P}[S_t^e = s^n])$; i.e. we have some probability that the state is $s^1$ or $s^n$, etc
        - Recurrent neural network: $S_t^a = \sigma(S_{t-1}^aW_s + O_tW_o)$, take a linear combination of the agent state you had at the last time step with your current observation to generate your new state
    - Formally, this is a *partially observable Markov decision process* (POMDP)
## Inside an RL agent

So far we've only talked about the problem, not yet how to solve the problem.

- An RL agent may include one or more of these components:

    - **Policy**: the agent's *behaviour function*
        - A map from state to action. 
        - Deterministic policy: If we're in some state $s$, we have some policy $\pi$ that then determines our action from that state, i.e. $a=\pi(s)$.
        - Stochastic policy: Can be useful, helps us make random, exploratory decisions, i.e. $\pi(a|s)=\mathbb{P}[A=a|S=s]$

    - **Value function**: how good is each state and/or action. How much reward do I expect to get if we take this action in this particular state.
        - A prediction of expected future reward. This is how we choose between action 1 and action 2.
        - Since the value will depend on what your action/policy is, we index by $\pi$: $v_\pi(s)=\mathbb{E}_\pi\left[R_t+\gamma R_{t+1} + \gamma^2 R_{t+2} + \dots|S_t=s\right]$
            - i.e. The value function tells us how much total reward we expect going into the future. We can also have discounting ($\gamma$) that goes into the future which helps us care more about immediate rewards if we want.

    - **Model**: agent's subjective representation of the how the environment works. Its sometimes useful to learn the behaviour of the environment and use that model of the environment to help figure out what to do next.
        - It is optional to do this; a lot of the course we'll focus on model-free methods that don't use a model at all.
        - The way we normally do this is to have two parts of the model:
            - Transition model: $\mathscr{P}$ predicts the next state, predicts the dynamics of the environment. If this were the helicopter, this is the function that would predict where the helicopter would be next given its current state.
                - Formally this is represented in a state transition model that is the probability of being in the next state given the previous state and action: $\mathscr{P}^a_{ss'}=\mathbb{P}[S'=s'|S=s,A=a]$
            - Reward model: $\mathscr{R}$ predicts the next (immediate) reward. The helicopter can learn that if its in this position then it's not crashing and therefore doing well.
                - Formally this is represented as a function that tells us the expected reward given the previous state and action: $\mathscr{R}^a_s=\mathbb{E}[R | S=s,A=a]$

> [!tip] Example: Maze
> ![[Pasted image 20230817132701.png]]
> 
> Here the goal is to navigate this maze as quickly as possible, so:
> - Reward: -1 for every second taken
> - Actions: N, E, S, W
> - State: Agent's location in the maze
> 
> An example of policy is like the following. For each state (location), there is an action (the arrows) mapped to it ($\pi(s)$)
> ![[Pasted image 20230817133000.png]]
> 
> An example of the value function ($v_\pi(s)$), each state has an expected future value attached. You can see that its -1 at the last state since it knows there's only one more step to the end. It can also go higher such as -24 when you've taken a wrong turn and now it'll take you more steps to get to the end.
> ![[Pasted image 20230817133131.png]]
> 
> An example of the agent's model of reality is like the following. The agent is trying to build its own map of the environment. The map represents the transition model ($\mathscr{P}^a_{ss'}$) and the numbers in each grid represent the expected reward in each state ($\mathscr{R}^a_s$)
> ![[Pasted image 20230817133422.png]]

### Categorising RL agents

- We can build a taxonomy of RL agents based on which of the key components our agent contains. The fundamental distinction in RL is whether the algorithm is model-free or model-based. Second to this is whether a policy or value function is used.
    - **Value based**: stores a value function. If it's got a value function then the policy is implicit - it just needs to pick actions greedily with respect to the value function
    - **Policy based**: we explicitly represent the policy and the agent will work on creating a policy that maximises the total cumulative reward - without ever storing an explicit value function
    - **Actor Critic**: stores both the policy and the value function
    - **Model Free**: has a policy and/or value; we do not try to explicitly understand the dynamics of the environment, we just see our policy/rewards and base our actions on that
    - **Model Based**: first step is to build a dynamics model of how the environment works. Has a policy and/or value

![[Pasted image 20230817134408.png]]

## Problems within reinforcement learning

### Learning and planning

Two fundamental problems in sequential decision making

- Reinforcement Learning:
    - The environment is unknown; the agent isn't told how the environment works.
    - The agent interacts with the environment with the aim of getting the most cumulative reward, trial and error.
    - The agent improves its policy.

- Planning:
    - A model of the environment is known, we tell the agent this
    - Instead of interacting with the environment, the agent performs internal computations with its model
    - As a result of this, the agent improves its policy.

You could also learn how the environment works, and then do planning

> [!tip] Example: Atari
> Reinforcement Learning setup: Rules of the game are unknown, the agent learns directly from interactive gameplay, picks actions on the joystick and sees pixels and scores.
> ![[Pasted image 20230817135332.png]]
> 
> Planning setup: Rules of the game are known and told to the agent, can query an emulator (consider this a perfect model in the agent's brain) to plan its next step using look-ahead or tree search.
> ![[Pasted image 20230817135436.png]]

### Exploration and exploitation

How do we balance exploration and exploitation?

- Reinforcement learning is like trial and error learning. 
    - The problem is that the agent is losing reward along the way. 
    - We want to figure out a good policy without giving up opportunities to exploit the things it has discovered.

- Exploration finds more information about the environment, giving up reward to do so. Could you potentially gain more reward by trying something you haven't done yet?
- Exploitation exploits known information to maximise reward
- It is important to do both, the question is what is the best balance?

> [!tip] Examples
> - Restaurant Selection
>     - *Exploitation*: go to your favourite restaurant
>     - *Exploration*: Try a new restaurant - you'll never know if you'll find 
> - Online Banner Advertisements
>     - *Exploitation*: Show the most successful advert
>     - *Exploration*: Show a different advert - which might be more successful

### Prediction and control

- Prediction: evaluate the future given a policy
- Control: what is the optimal policy, what policy should you choose?

Typically you need to solve the prediction problem in order to solve the control problem

> [!tip] Example: GridWorld
> Prediction setup: if we perform a fixed policy of moving randomly across the grid, how much reward will we get?
> ![[Pasted image 20230817141004.png]]
> 
> Control setup: whats the optimal behaviour of this GridWorld? If I behave optimally, now what's the value function? This is very different to the prediction problem
> ![[Pasted image 20230817141117.png]]