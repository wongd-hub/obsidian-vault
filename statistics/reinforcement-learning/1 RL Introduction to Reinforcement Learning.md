#course_google-deepmind-reinforcement-learning #reinforcement-learning

Website: davidsilver.uk/teaching
## About reinforcement learning

![[Pasted image 20230816152354.png]]

- *Reinforcement learning* (RL) sits at the intersection of many different fields of science. Many different fields try to understand the optimal way to make decisions, and these all relate to reinforcement learning.

![[Pasted image 20230816152543.png]]

- How is RL distinct from supervised and unsupervised learning?
    - There is no supervisor, only a reward signal
    - Feedback is delayed, possibly by several steps - and is not instantaneous
    - Time actually matters, we're looking at sequential processes in which an agent moves throughout a world which means non-i.i.d data
    - The agent's actions can influence its environment and therefore the subsequent data it sees

> [!tip] Examples of RL use cases
> - **Fly stunt manoeuvres in a helicopter** - a reward if the manoeuvre is performed successfully with no crashes
> - **Defeat the world champion in Backgammon or play Atari games better than humans** - good reward if a game is won
>    - Don't even need to tell it the rules of the game, just set up the reward structure
> - **Manage an investment portfolio** - the reward might be financial returns
> - **Make a humanoid robot walk** - rewards for making progress

## The reinforcement learning problem
### The reward

- A *reward* $R_t$ - a scalar feedback signal. Indicates how well the agent is doing at step *t*. The agent's job is to maximise cumulative reward.
    - The overarching goal is to select actions to maximise total future reward.
        - Actions might have long term consequences
        - Reward may be delayed
        - It may be better to sacrifice immediate reward to gain more long-term reward, strategic thinking is part of the process
    - What if the goal is to perform an action in the fastest amount of time? Typically what's done is to define the reward as `-1` per time step.
    - You can use multi-faceted reward systems, but you can generally represent these in a summarised fashion as a scalar reward.

> [!info] Definition (Reward Hypothesis)
> All goals can be described by the *maximisation* of *expected cumulative reward*.
> 
> - Reinforcement learning is based on this hypothesis.

> [!tip] Examples of $R_t$
> - **Fly stunt manoeuvres in a helicopter**: +ve for following desired trajectory, (large) -ve for crashing
>     - Re-fueling itself might prevent a crash in several hours (stop and lose a small amount of reward for not following path to mitigate a crash later due to no fuel)
> - **Defeat the world champion at Backgammon**: no intermediate rewards, but at end of game give a +/-ve signal based on winning/losing the game
> - **Manage an investment portfolio signal**: +ve reward for each $ in bank
>     - Investment decisions might take months to come to fruition (delayed reward)
> - **Controlling a power station**: +ve reward for producing power, -ve reward for exceeding safety thresholds
> - **Making a humanoid robot walk**: +ve reward for forward motion, (large) -ve reward for falling over
> - **Making a car go around a track**: -ve reward for exceeding track limits/moving x% away from designated racing line, -ve reward for every second taken to get around the track
>     - The former needs to be big enough that the optimal strategy isn't to exceed track limits to gain time

### Agent and the environment

![[Pasted image 20230816155812.png]]

- The brain represents the agent, this is what we're building. Our agent will be composed of algorithms that will be able to respond to stimuli and take actions (such as move controls, or make investments). So at each step the agent will:
    - Execute action $A_t$
    - Receive observation $O_t$
    - Receive scalar reward $R_t$

- The environment is represented by the world (what's on the other side of the agent). There will be a loop over time, where in every step the agent sees a snapshot of the world at this moment generated by the environment. We have no control over the environment except through the agent's action $A_t$. So the environment will:
    - Receive action $A_t$
    - Emit observation $O_t$
    - Emit scalar reward $R_t$

