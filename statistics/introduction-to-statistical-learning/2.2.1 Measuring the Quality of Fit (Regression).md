#textbook-intro-to-statistical-learning 

- We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. 
- In the regression setting, the most commonly-used measure is the mean squared error (MSE)
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2$$
* In general, we don’t really care how well the method works on the training data. Instead, we’re interested in the accuracy of predictions on data that was previously unseen by the model.

- As the flexibility of the statistical method increases, we tend to observe the accuracy over the training set trending monotonically downwards; whereas the accuracy over the test set will decrease initially but hit an inflection point and then increase.  ^8196c6
	- This is a fundamental property of statistical learning.
	- When the model has a low MSE for the training data but a high test MSE, the model is said to be *overfitting* the data.
	- This may happen because the statistical learning algorithm is working too hard to find patterns in the training data, and may be picking up patterns that are caused by random chance instead of the true properties of $f$.
	- We generally expect the training MSE to be lower than the test MSE regardless of if overfitting has occurred because the model seeks to minimise MSE on the training set.

- Methods will be discussed later for picking the correct training/test sets to most accurately find which model minimises test MSE. This will include methods such as [[cross-validation]].
