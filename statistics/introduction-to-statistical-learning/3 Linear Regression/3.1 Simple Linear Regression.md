#simple-linear-regression #regression 

- Straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes there is a linear relationship between X and Y at the population level, ie. that the following formula holds
$$Y \approx \beta_0 + \beta_1X$$
> [!info]
> This can be referred to as *regressing* $Y$ *on/onto* $X$

- $\beta_0$ and $\beta_1$ are two unknown constants (i.e. model parameters) that represent the *intercept* and *slope* of the linear model.
  - We will use our training data to estimate $\hat{\beta}_0$ and $\hat{\beta}_1$, ie. the intercept and slope derived from our sample.
  - We can then estimate what the response will be for a given value of $x$ using:
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$$
> [!info]
> We use the caret to denote an *estimated* value for an unknown parameter or coefficient (as opposed to the population value)

## 3.1.1 Estimating the Coefficients

- In practice, the population $\beta_0$ & $\beta_1$ are unknown. We must use data to estimate these coefficients.
- Our goal is to obtain coefficient estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ such that the linear model fits the available data well (ie. $y_i \approx \hat{\beta}_0 + \hat{\beta}_1x_i$ for $i = 1, \ldots ,n$)
  - Put simply, we want to find intercept $\beta_0$ and slope $\beta_1$ such that the resulting line fits as *close as possible* to the *n* training data points.
  - There are many ways of measuring closeness, but we’ll be focusing on minimising the *least squares* criterion in this chapter.

- $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$ is the prediction for Y based on the *i*th value of X.
  - Then $e_i = y_i - \hat{y}_i$ is the *i*th *residual*; the distance between the observed value ($y_i$) and the predicted value (\hat{y}_i)
  - Summing and squaring (so that negative/positive residuals don’t cancel each other out) all residuals, we get the *residual sum of squares*. This is a measure of the total distance between modelled and observed values for the dataset.
  - ==The least squares approach to linear regression aims to find $\hat{\beta}_0$ and $\hat{\beta}_1$ such that residual sum of squares is minimised.==

> [!info] Residual Sum of Squares (RSS)
> $$RSS = e_1^2 + e_2^2 + \ldots + e_n^2$$
> Expanding with $e_i = y_i - \hat{y}_i$ & $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$ we get:
> $$RSS = (y_1 - \hat{\beta}_0 - \hat{\beta}_1x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1x_2)^2 + \ldots + (y_n - \hat{\beta}_0 - \hat{\beta}_1x_n)^2$$

> [!info] Least Squares approach to Regression
> Using calculus, one can show that the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimise the RSS are:
> $$\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$
> $$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$$
> Where $\bar{y} \equiv \frac{1}{n}\sum_{i=1}^{n} y_i$ and $\bar{x} \equiv \frac{1}{n}\sum_{i=1}^{n} x_i$ are the sample means.

^40a4e4
## 3.1.2 Assessing the Accuracy of the Coefficient Estimates

- In linear regression we assume the *true* relationship between X and Y is linear and hence takes the form $Y = f(X) + \epsilon =\beta_0 + \beta_1X + \epsilon$. This is the *population* regression line, ==our best linear approximation to the true relationship between X and Y==.
  - $\beta_0$ and $\beta_1$ are the intercept and slope estimates. $\epsilon$ is the catch-all (and mean-zero) error term for what we miss by approximating reality with this simple model. 
  - The true relationship is likely not linear, we may be missing other X variables that cause variation in Y, and there might be measurement error.
- The [[#^40a4e4|least squares estimates]] for $\beta_0$ and $\beta_1$ define the *least squares* line.

- We can see the difference between the two in the graph below. The red line is the *population line* plotted from a known linear relationship ($Y = 2 + 3X$). The blue line is the *least squares* line fit on data generated from that linear relationship *plus* the $\epsilon$ error term which is normally distributed with mean zero.
  - Further, the light blue lines in the graph on the right are least squares lines fit on samples of the generated data.

![](https://lh3.googleusercontent.com/J_H-NEQ6QEDM65B9crCrA-1_Jz4fKp2KKqPhjv6O9BgX1sCgnHE3KRou6zqNw-kVvqaqcceDHxAhNclFyg3zQ0o22WmqeCKsH-IL9dRsQa6dtuXDN4wCTN1ttmm2uVQoqzitfzuiJ4kKBb66cbwRqJI)

> [!info] Why are there multiple lines?
> - In practice, the population linear relationship (red line) is not known and must be approximated with data (any of the blue lines). 
> - Fundamentally, this is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.
> ----
> - For example, say we’re interested in knowing the population mean $\mu$ of some random variable $Y$.
> - Unfortunately, $\mu$ is unknown, but we do have access to $n$ observations from $Y$, $y_1, \dots, y_n$ from which we can reasonably estimate $mu \approx \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ (we are estimating the population mean using the sample mean).
> - In other words, the sample mean and population mean are different, but the sample mean provides a good approximation of the population mean. This same concept applies to the approximation of $\beta_0$ and $\beta_1$ using $\hat{\beta_0}$ and $\hat{\beta_1}$ estimated from sample data.

### Bias & Variance
#### Unbiased estimator

- $\hat{\beta_0}$ and $\hat{\beta_1}$ are *unbiased estimates* of the true population parameters, which means that over multiple samples, the estimates will on average equal the true parameter.
  - I.e. These estimates will not systematically underor over-estimate the true population parameters.
#### Standard error

- The natural follow-up question is how accurate are $\hat{\beta_0}$ and $\hat{\beta_1}$ as estimates of $\beta_0$ and $\beta_1$? We answer this question by computing the *standard error* of $\hat{\mu}$ ($SE\left(\\hat{\mu}\right)$).
  - The standard error tells us the average amount that the parameter estimates differ from the population parameter value.

> [!info] Standard error of the population mean
> $$\text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n}$$

- Where $\sigma$ is the standard deviation of each of the realisations $y_i$ of $Y$.
  - The equation above also tells us that the deviation shrinks with $n$ - more observations means smaller standard error.
  - $\sigma$ and $\sigma^2$ are population parameters and are not known, but can be estimated from the data.

> [!info] Residual standard error (our estimate of $\sigma$)
> $$\hat{\sigma} = RSE = \sqrt{\frac{RSS}{n - 2}}$$
> Where RSS is the [[Residual Sum of Squares]] - the total distance between modelled and observed values for the dataset

- The equations for standard error of $\hat{\beta_0}$ and $\hat{\beta_1}$ are:

> [!info] Standard error of the intercept and slope coefficients
> $$\text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$$
> $$\text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

> [!info] Intuition
> - $\text{SE}(\hat{\beta}_1)^2$ (standard error for the slope estimate) is smaller when the $x_i$ are more spread out. Intuitively, we have more leverage to estimate the slope when this is the case.
> - $\text{SE}(\hat{\beta}_0)^2$ (standard error for the intercept estimate) is the same as $\text{SE}(\hat{\mu})$ if $\bar{x} = 0$. If this were the case, $\hat{\beta}_0)$ would equal $\bar{y}$.
### Confidence intervals

- We can use standard errors to compute [[2 Confidence Intervals|confidence intervals]].
  - A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.
  - In other words, if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter.

- In linear regression, the 95% confidence intervals for $\beta_1$ approximately takes the form:

> [!info] Confidence intervals for $\beta_0$ and $\beta_1$
> $$\beta_1 \pm 2 \cdot \text{SE}(\hat{\beta}_1)$$
> $$\hat{\beta}_0 \pm 2 \cdot \text{SE}(\hat{\beta}_0)$$
> 
> - Note, to be precise, rather than the number 2, these equations should contain the 97.5% quantile of the $t$-distribution with $n-2$ degrees of freedom.
### Hypothesis tests

- Standard errors can also be used to perform [[3 Hypothesis Testing|hypothesis tests]], a concept related to confidence intervals.
- The most common hypothesis test for linear regression is testing whether there is any evidence of a relationship between X and Y. i.e.:
$$H_0 : \beta_1 = 0$$
$$H_a : \beta_1 \neq 0$$
- If $\beta_1 = 0$ then the model reduces down to the intercept and error term, and X is not associated with Y.
  - To test this, we need to determine whether $\hat{\beta_1}$ is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero.
  - How far is far enough? That depends on the accuracy of $\hat{\beta_1}$ (hence $\text{SE}(\hat{\beta}_1)$). If the standard error is small, then even small values of $\hat{\beta}_1$ may provide strong evidence that $\beta_1 \neq 0$. In contrast, if $\text{SE}(\hat{\beta}_1)$ is large, then $\hat{\beta}_1$ must be large in order for us to reject the null hypothesis.

- In practice, to perform this test, we calculate a *t-statistic* given by:

> [!info] *t*-statistic
> $$t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}$$

- Conceptually, this measures how many standard deviations $\hat{\beta}_1$ is away from 0. 
  - If the coefficient is large compared to its standard error, the *t*-statistic will be high, leading to low *p-values*.

- If there really is no relationship between X and Y (i.e. $\beta_1 = 0$), we’d expect that the *t-statistic* will have a *t*-distribution with $n-2$ degrees of freedom (since we’re estimating two parameters).
  - The *t*-distribution has a bell shape and for $n > 30$ it is similar to the standard normal distribution.
  - In this case, it is easy to compute the probability of observing any number equal to $|t|$ or larger in absolute value, assuming that $\beta_1 = 0$. This is the [[3 Hypothesis Testing#p-value|p-value]].
  - We interpret the *p-value* as follows: a small p-value indicates that it is unlikely to observe such a substantial association between X and Y due to chance, in the absence of any real association between the predictor and the response.
  - Hence if the p-value is small (typically below 5%) we *reject the null hypothesis*.

## 3.1.3 Assessing the Accuracy of the Model

- Once we have rejected the null hypothesis (i.e. we accept that there is an association between X and Y), it is natural to want to quantify the extent to which the model fits the data.
- The quality of a linear regression fit is assessed using two related quantities: the *residual standard error* (RSE) and the $R^2$ statistic. The *F*-statistic is also used but will be discussed in [[3.2.2]].
### Residual Standard Error 

- Recall that the error associated with each observation/prediction is $\epsilon$
- Due to the presence of these errors, even if we knew the true regression line (i.e. even if the population parameters $\beta_0$ and $\beta_1$ are known), we would not be able to perfectly predict Y from X.
- RSE is an estimate of the standard deviation of $\epsilon$, i.e. ==it is the average amount that the response will deviate from the true regression line==.
  - It is considered a measure of the *lack of fit of a model* to the data.
$$RSE = \sqrt{\frac{1}{n - 2} RSS} = \sqrt{\frac{1}{n - 2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
$$RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
- Say for example our RSE is 3,260. This means that even if the model were correct and the true values of the unknown coefficients $\beta_0$ and $\beta_1$ were known exactly, any prediction would still be off by 3,260 on average.


> [!info] Intuition
> - If the predictions obtained by the model are very close to the observed values (i.e. $\hat{y_i} \approx y_i$) then the RSE will be small and we can conclude the model fits the data well.
> - Given that RSE is measured in units of the response, Y, it is not always clear what constitutes a good RSE.
### R^2 Statistic

- The $R^2$ statistic provides an alternative measure of fit that takes the form of a proportion; ==the proportion of variance explained==.
$$R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}$$
$$TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2$$
$$RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- The *Total Sum of Squares* (TSS) measure the total variance in the response Y (the amount of variability inherent in the data before regression is performed).
- RSS measures the amount of variability left unexplained after performing regression.

> [!info] Intuition
> - This means that the $R^2$ statistic measures the total amount of variability that has been explained by the predictors.
> - A low $R^2$ might occur because the linear model is wrong or the error variance $\sigma^2$ is high or both.
> - This measure is easier to interpret than the *RSE* but it can still be challenging to determine what a *good* $R^2$ is and in general this will depend on the application (and how linear we think the actual data should be).

- The $R^2$ statistic is a measure of the linear relationship between X and Y; recall that *correlation* is another measure of that linear relationship.
$$\text{Cor}(X, Y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$
- This suggests that we might be able to use $r = \text{Cor}\left(X, Y\right)$ instead of $R^2$ to assess the fit of the linear model. It can be shown that, in the simple regression setting, $R^2 = r^2$.
  - The concept of correlation between predictors and the response does not extend automatically to the multiple regression setting (since correlation quantifies the association between a single pair of variables rather than between a larger number of variables). $R^2$ can be useful in this case.