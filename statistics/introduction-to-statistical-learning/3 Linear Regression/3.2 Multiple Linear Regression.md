#multiple-linear-regression #regression 

- When modelling something that is influenced by multiple predictors, fitting a separate simple linear regression model for each predictor is not sufficient as each model ignores the other predictors. This can lead to issues of spurious or misleading associations between a predictor and the response that may arise when predictors are correlated.
- Instead of using one predictor, multiple linear regression uses multiple predictors. It does this by giving each predictor a separate slope coefficient in a single model.Â 
- Instead of using separate simple linear regression models for each predictor, a better approach is to extend the simple linear regression model to accommodate multiple predictors directly. The multiple linear regression model with $p$ distinct predictors takes the form:
$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p + \epsilon $$
- Where $X_j$ represents the $j$-th predictor and $\beta_j$ quantifies the association between that variable and the response $Y$. The coefficient $\beta_j$ represents the average effect on $Y$ of a one unit increase in $X_j$, holding all other predictors fixed.

- For example, in the Advertising data, we have examined the relationship between sales and TV advertising, and we also have data for radio and newspaper advertising. To understand whether these two additional predictors are associated with sales, we need to use a multiple linear regression model:
$$ \text{sales} = \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{radio} + \beta_3 \times \text{newspaper} + \epsilon $$
## 3.2.1 Estimating the regression coefficients 

- In multiple linear regression, the model with $p$ predictors takes the form:
$$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon \quad \text{(equation 3.19)} $$
- Where $X_j$ represents the $j$-th predictor, $\beta_j$ quantifies the association between that predictor and the response $Y$, and $\epsilon$ is the error term.

- To estimate the regression coefficients $\beta_0, \beta_1, \ldots, \beta_p$, we use the least squares approach. The goal is to find the values of $\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p$ that minimise the sum of squared residuals (RSS):
$$ \text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \hat{\beta}_2 x_{i2} - \ldots - \hat{\beta}_p x_{ip})^2 \quad \text{(equation 3.22)} $$
- Where $\hat{y}_i$ is the predicted value of $Y$ for the $i$-th observation based on the estimated coefficients.

![](https://lh3.googleusercontent.com/Fth10domzk7Dm9vE-Vx3e3GOl4wxobpLixV-WsOSymd3nBHRXopNivc1HoR0vEXYAf4bG6bCEP1YWE8Zkl-UjPuPqnZmccleJA2WiMYv52nxkGDQc79T0VLgprdp1o3hbbMWEAooLUKv2CcDkmqM2tc)

> [!info]
> - In multiple regression, it's possible to find no direct relationship between a predictor and the response variable, while a simple linear regression might suggest otherwise. 
> - This can occur when predictor variables are correlated with each other. The correlated predictors can make it seem like a particular variable has an impact on the response, but its influence disappears when considering all predictors together. This phenomenon is common in real-life situations, and it underscores the importance of using multiple regression to properly analyse and interpret the individual effects of predictors while accounting for correlations between them.

## 3.2.2 Some important questions

