#textbook-intro-to-statistical-learning 

* Many methods try to estimate the conditional distribution of Y given X, and then classify an observation to the class with the highest estimated probability. One such method is the K-nearest neighbours (KNN) classifier.
* Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$, represented by $N_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$:
$$Pr(Y = j|X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)$$
* Finally, KNN classifies the test observation $x_0$ to the class with the largest probability from the above equation.

> [!info]
> For a highly non-linear Bayes decision boundary, we would expect that a smaller $K$ provides a better fit, since this gives the model more flexibility. A high $K$ would provide more bias but less variance.