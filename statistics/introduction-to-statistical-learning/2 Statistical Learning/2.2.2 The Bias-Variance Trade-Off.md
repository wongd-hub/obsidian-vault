#textbook_intro-to-statistical-learning

- It is possible to show that the expected test MSE (for a given value $x_0$) can be decomposed into the sum of the *variance* of $\hat{f}(x_0)$, the squared *bias* of $\hat{f}(x_0)$ and the variance of the irreducible error terms $\epsilon$. i.e.: ^3872be

$$E\left(y_0-\hat{f}(x_0)\right)^2=\text{Var}\left(\hat{f}(x_0)\right)+\left[\text{Bias}\left(\hat{f}(x_0)\right)\right]^2+Var\left(\epsilon\right)$$
- Where $E\left(y_0-\hat{f}(x_0)\right)^2$ is the expected test MSE at $x_0$ (the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$)
- From this equation, we can see that to minimise the expected test error, we need to achieve a low variance and low bias.

> [!info]
> The variance is inherently a non-negative quantity, and the bias is squared so must always be positive. This means that expected test MSE can never lie below $\text{Var}\left(\epsilon\right)$.

- What do we mean by the *variance* and *bias* of a statistical learning method?
    - Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training set.
    - Since the training data are used to fit the statistical learning method, if we change the training data then the model estimate will also change. However, ideally we’d want it to not vary too much between training sets (because if it **did** vary a lot, that means we’re fitting the peculiarities in the data as opposed to the underlying $f$).
    - In general, more flexible statistical methods have higher variance.

- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. E.g. Assuming that there is a linear relationship in the data whereas in real life the relationship is likely to be slightly non-linear.
    - As we use more flexible methods, the variance will increase but the bias will decrease. Generally, as we increase the flexibility, bias decreases faster than the variance increases - but at some point variance will start to increase much faster.
    - The lowest MSE will sit around where the decrease in in the bias has slowed down, and the increase in the variance hasn’t sped up yet.
    - This relationship is referred to as the *bias-variance* trade-off.

![[Pasted image 20230723122348.png]]

> [!info]
> In a real-life situation in which $f$ is unobserved, it is generally not possible to explicitly compute the test MSE, bias, nor variance for a statistical learning method.
